{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Medicare Fraud in the US using Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical ID Fraud is a prevalent but not often talked about issue in the US.\n",
    "\n",
    "Forbes estimates that fraudulent medical accounts make up 3-10% of the entire multi-billion US healthcare system every year.\n",
    "\n",
    "- Combining all years for CMS (2012-2017) and LEIE (01/2018-12/2019).\n",
    "\n",
    "\n",
    "This investigation follows Johnson & Khoshgoftaar in cleaning and aggregating data. I used 2 main datasets including "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='menu'></a>\n",
    "### Menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cms'>1. Pull CMS Data</a>\n",
    "\n",
    "- <a href='#leie'>2. Pull LEIE Data</a>\n",
    "\n",
    "- <a href='#combined'>3. Combine and Label Fraud</a>\n",
    "\n",
    "- <a href='#visualization'>4. Build Visualizations</a>\n",
    "    - <a href='#usmap'>US Medicare Fraud Map</a>\n",
    "    - <a href='#histogram1'>Histograms of Submitted Charge/Mediacre Reimbursements Comparing Fraud vs. Non-Fraud</a>\n",
    "    - <a href='#correlation1'>Correlation Heatmap for Continuous Variables </a>\n",
    "    - <a href='#correlation2'>Table of High Correlations</a>\n",
    "    - <a href='#barchart1'>Top 15 Fraudulent Medicare Category by Service Count</a>\n",
    "    - <a href='#barchart2'>Top 15 Fraudulent Medicare Category by Service Count & Gender</a>\n",
    "\n",
    "- <a href='#train'>5. Prep Data for Training</a>\n",
    "    - <a href='#norm'>Choose Features & One-hot Encoding</a>\n",
    "    - <a href='#ros'>Random Undersampling</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directory\n",
    "CWD = os.getcwd()\n",
    "cms_data_dir = os.path.join(CWD, 'CMSData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some years columns are capitalized and other years the columns are lowercase:\n",
    "capitalization_dict = {\n",
    "    '2012': str.upper,\n",
    "    '2013': str.upper,\n",
    "    '2014': str.lower,\n",
    "    '2015': str.lower,\n",
    "    '2016': str.upper,\n",
    "    '2017': str.lower,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='cms'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CMS Part B dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dtypes based on https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/...\n",
    "#Medicare-Provider-Charge-Data/Physician-and-Other-Supplier2017\n",
    "partB_dtypes = {\n",
    "    'npi': 'str',\n",
    "    'nppes_provider_last_org_name': 'str',\n",
    "    'nppes_provider_first_name': 'str',\n",
    "    'nppes_provider_mi': 'str',\n",
    "    'nppes_credentials': 'str',\n",
    "    'nppes_provider_gender': 'str',\n",
    "    'nppes_entity_code': 'str',\n",
    "    'nppes_provider_street1': 'str',\n",
    "    'nppes_provider_street2': 'str',\n",
    "    'nppes_provider_city': 'str',\n",
    "    'nppes_provider_zip': 'str',\n",
    "    'nppes_provider_state': 'str',\n",
    "    'nppes_provider_country': 'str',\n",
    "    'provider_type': 'str',\n",
    "    'medicare_participation_indicator': 'str',\n",
    "    'place_of_service': 'str',\n",
    "    'hcpcs_code': 'str',\n",
    "    'hcpcs_description': 'str',\n",
    "    'hcpcs_drug_indicator': 'str',\n",
    "    'line_srvc_cnt': 'float64',\n",
    "    'bene_unique_cnt': 'float64',    \n",
    "    'bene_day_srvc_cnt': 'float64',\n",
    "    'average_medicare_allowed_amt': 'float64',\n",
    "    'average_submitted_chrg_amt': 'float64',\n",
    "    'average_medicare_payment_amt': 'float64',\n",
    "    'average_medicare_standard_amt': 'float64',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dfs for all years - TAKE A FEW MINUTES\n",
    "years = ['2012','2013','2015','2016']\n",
    "dfs   = []\n",
    "\n",
    "for year in years:\n",
    "    file = os.path.join(cms_data_dir, f'cms{year}.txt')\n",
    "    dtypes = dict(zip(list(map(capitalization_dict[year], partB_dtypes.keys())), list(partB_dtypes.values()))) #get correct column capitalization and dtype\n",
    "    df = pd.read_csv(file, delimiter='\\t', dtype=dtypes)\n",
    "    df.columns = map(str.lower, df.columns)  # make all variable names lowercase\n",
    "    df['year'] = year #add Year column \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37653939, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate\n",
    "partB_df = pd.concat(dfs, axis=0, ignore_index=True, sort=False)\n",
    "partB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows corresponding to drugs because LINE_SRVC_CNT for them is not a desirable count\n",
    "partB_df = partB_df[(partB_df['hcpcs_drug_indicator'] == 'N')]\n",
    "partB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing NPI and HCPCS - \"Medicare fraud detection using neural networks\" (Johnson, Khoshgoftaar 2019)\n",
    "# This means dropping 2014 and 2016 - both did not have HCPCS Code\n",
    "partB_df = partB_df.dropna(subset = ['npi','hcpcs_code'])\n",
    "partB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep variables based on \"Medicare fraud detection using neural networks\" (Johnson, Khoshgoftaar 2019)\n",
    "partB_variables_to_keep = [\n",
    "    'npi',\n",
    "    'provider_type',\n",
    "    'nppes_provider_city', # keep\n",
    "    'nppes_provider_zip', # keep\n",
    "    'nppes_provider_state', # keep\n",
    "    'nppes_provider_country', # keep\n",
    "    'hcpcs_code',  # not in paper but kept\n",
    "    'hcpcs_description',  # not in paper but kept\n",
    "    'hcpcs_drug_indicator',  # not in paper but kept\n",
    "    'place_of_service',  # not in paper but kept\n",
    "    'nppes_provider_gender',\n",
    "    'line_srvc_cnt',\n",
    "    'bene_unique_cnt',\n",
    "    'bene_day_srvc_cnt',\n",
    "    'average_submitted_chrg_amt',\n",
    "    'average_medicare_payment_amt',\n",
    "    'year' # need Year for labeling\n",
    "]\n",
    "partB_df = partB_df[partB_variables_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partB_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partB_df.loc[partB_df['npi'] == '1003000142'][['npi',\n",
    "                                             'provider_type',\n",
    "                                             'place_of_service',\n",
    "                                             'line_srvc_cnt',\n",
    "                                             'average_submitted_chrg_amt',\n",
    "                                             'year']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partB_df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all combined CMS to csv\n",
    "#partB_df.to_csv('combined-partB-data-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='leie'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LEIE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leie_data_dir = os.path.join(CWD, 'LEIEData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leie_dtypes = {\n",
    "    'LASTNAME': 'str',\n",
    "    'FIRSTNAME': 'str',\n",
    "    'MIDNAME': 'str',\n",
    "    'BUSNAME' : 'str',\n",
    "    'GENERAL': 'str',\n",
    "    'SPECIALTY': 'str',\n",
    "    'UPIN': 'str',\n",
    "    'NPI': 'str',\n",
    "    'DOB': 'str',\n",
    "    'ADDRESS': 'str',\n",
    "    'CITY': 'str',\n",
    "    'STATE': 'str',\n",
    "    'ZIP': 'str',\n",
    "    'EXCLTYPE': 'str',\n",
    "    'EXCLDATE': 'int64',\n",
    "    'REINDATE': 'int64',\n",
    "    'WAIVERDATE': 'int64',\n",
    "    'WVRSTATE': 'str',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEIE data is monthly between 01/2018 (1801) - 12/2019 (1912)\n",
    "year_months = ['1801','1802','1803','1804','1805','1806','1807','1808','1809','1810','1811','1812',\n",
    "            '1901','1902','1903','1904','1905','1906','1907','1908','1909','1910','1911','1912']\n",
    "dfs = []\n",
    "\n",
    "for year_month in year_months:\n",
    "    file = os.path.join(leie_data_dir, f'leie{year_month}-excl.csv')\n",
    "    df   = pd.read_csv(file, dtype=leie_dtypes)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "leie_df = pd.concat(dfs, axis=0, ignore_index=True, sort=False)\n",
    "leie_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NPI = 0, which means missing - A LOT ARE MISSING, which is a problem for the data\n",
    "leie_df = leie_df[leie_df['npi'] != 0]\n",
    "leie_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep exclusions most related to Fraud\n",
    "exclusions_to_keep = [\n",
    "    '1128a1',\n",
    "    '1128a2',\n",
    "    '1128a3',\n",
    "    '1128b4',\n",
    "    '1128b7',\n",
    "    '1128c3Gi',\n",
    "    '1128c3gii',\n",
    "]\n",
    "leie_df = leie_df[leie_df['excltype'].isin(exclusions_to_keep)]\n",
    "leie_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leie_df['excltype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1128a1: Conviction of program-related crimes\n",
    "- 1128a2: Conviction of relating to patient abuse or neglect\n",
    "- 1128a3: Felony conviction relating to healthcare fraud\n",
    "- 1128a4: License revocation, suspension, or surrender\n",
    "- 1128a3: Fraud, kickbacks, other prohibited activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all combined LEIE to csv\n",
    "#partB_df.to_csv('combined-leie-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='combined'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine/Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "leie_df['excldate'] = pd.to_datetime(leie_df['excldate'], format='%Y%m%d', errors ='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round excl date to the nearest year Johnson & Khoshgoftaar (2019)\n",
    "def round_to_year(dt=None):\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    if month >= 6:\n",
    "        year = year + 1\n",
    "    return datetime(year=year,month=1,day=1)\n",
    "\n",
    "leie_df['excl_year'] = leie_df.excldate.apply(lambda x: round_to_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make exclusion dict \n",
    "# 1215053665 has 2 exclusions, so sort also df to get latest year\n",
    "excl_year_dict = dict([npi, year] for npi, year in zip(leie_df.sort_values(by='excl_year').npi, leie_df.sort_values(by='excl_year').excl_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get label as 0 or 1\n",
    "partB_df['excl_year'] = partB_df['npi'].map(excl_year_dict)\n",
    "partB_df['excl_year'] = partB_df['excl_year'].fillna(datetime(year=1900,month=1,day=1)) # fill NaN, physicians without exclusion, with year 1900\n",
    "\n",
    "partB_df['year'] = pd.to_datetime(partB_df['year'].astype(str), format='%Y', errors ='ignore')\n",
    "partB_df['fraudulent'] = np.where(partB_df['year'] < partB_df['excl_year'], 1, 0) # compare year vs. exclusion year to get Fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"partB_df is our combined dataset with shape: {0}\".format(partB_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='visualization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Draw Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects  as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number and amount of fraudulent services\n",
    "partB_df['fraud_line_srvc_cnt'] = partB_df['line_srvc_cnt']*partB_df['fraudulent']\n",
    "partB_df['fraud_average_submitted_chrg_amt'] = partB_df['average_submitted_chrg_amt']*partB_df['fraudulent']\n",
    "partB_df['fraud_average_medicare_payment_amt'] = partB_df['average_medicare_payment_amt']*partB_df['fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by state\n",
    "state_df = partB_df.groupby('nppes_provider_state').agg({\n",
    "    'line_srvc_cnt':[('total_services_count','sum')],\n",
    "    'fraud_line_srvc_cnt':[('total_fraud_services_count','sum')]\n",
    "}).reset_index()\n",
    "\n",
    "# Drop multi-index\n",
    "state_df.columns = ['_'.join(col) for col in state_df.columns]\n",
    "state_df.columns = ['provider_state', 'total_services_count', 'fraud_services_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get % fraud\n",
    "state_df['fraud_services_pct'] = state_df['fraud_services_count']/state_df['total_services_count']\n",
    "state_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='usmap'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=state_df['provider_state'],\n",
    "    z = np.log(state_df['fraud_services_pct'].astype(float))+0.000001, #log-scale\n",
    "    locationmode = 'USA-states',\n",
    "    colorscale = 'Reds',\n",
    "    colorbar_title = \"Logged %\",\n",
    "    marker_line_color='white'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Logged Percentage of Medicare Fraudulent Service by US State (2012-2016)',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat map shows a high concentration of fraudulent Medicare claims around the Northeast and South of the US, with Texas leading all states by a wide margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variables for gender\n",
    "partB_df2 = pd.concat([partB_df, pd.get_dummies(partB_df['nppes_provider_gender'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by provider type\n",
    "type_df = partB_df2.groupby('provider_type').agg({\n",
    "    'line_srvc_cnt':['sum'],\n",
    "    'fraud_line_srvc_cnt':['sum'],\n",
    "    'M':['sum'],\n",
    "    'F':['sum'],\n",
    "    'average_submitted_chrg_amt':['median'], #since distribution skewed right\n",
    "    'average_medicare_payment_amt':['median'],\n",
    "    'fraud_average_submitted_chrg_amt':['max'],\n",
    "    'fraud_average_medicare_payment_amt':['max']\n",
    "}).reset_index()\n",
    "\n",
    "# Drop multi-index\n",
    "type_df.columns = ['_'.join(col) for col in type_df.columns]\n",
    "type_df.columns = ['provider_type', 'total_services_count', 'fraud_services_count','male_count','female_count',\n",
    "                   'avg_submitted_chrg_amt','avg_medicare_payment_amt', 'fraud_avg_submitted_chrg_amt','fraud_avg_medicare_payment_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "type_df = type_df.sort_values('fraud_services_count',ascending=False)[:15] #get top 15 fraudulent types\n",
    "type_df = type_df.sort_values('total_services_count',ascending=True).reset_index(drop=True) #re-sort by total services\n",
    "\n",
    "# Add some fields\n",
    "type_df['non_fraud_services_count'] = type_df['total_services_count'] - type_df['fraud_services_count']\n",
    "type_df['fraud_services_pct'] = (type_df['fraud_services_count']/type_df['total_services_count'])*100\n",
    "type_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='histogram1'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get 2015 data only for speed\n",
    "partB_2015_df = partB_df2[partB_df2['year'] == datetime(year=2015,month=1,day=1)]\n",
    "top_7_types  = type_df['provider_type'][:7].tolist()\n",
    "\n",
    "for p_type in top_7_types:\n",
    "    \n",
    "    #Eliminate 0 payments then log\n",
    "    x = np.log(partB_2015_df[(partB_2015_df.provider_type == p_type) & (partB_2015_df.average_submitted_chrg_amt!=0)]['average_submitted_chrg_amt'])\n",
    "    fraud_x = np.log(partB_2015_df[(partB_2015_df.provider_type == p_type) & (partB_2015_df.fraud_average_submitted_chrg_amt!=0)]['fraud_average_submitted_chrg_amt'])\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    sns.distplot(x,label='Non-fraudulent', hist=False, rug=False)\n",
    "    sns.distplot(fraud_x,label='Fraudulent', hist=False, rug=False)\n",
    "\n",
    "    ax.set(\n",
    "        title  ='Distribution of Submitted Charge Amount - Fraud vs. Non-fraud - '+ p_type,\n",
    "        xlabel = 'Log of USD Payment Amount',\n",
    "        ylabel = 'Count'\n",
    "          )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious evidence to say that the average fraudulent submitted charge is more expensive than non-fraudulent charge across the Top 15 Fraudulent categories, as shown in histogram comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p_type in top_7_types:\n",
    "    #Eliminate 0 payments\n",
    "    x = partB_2015_df[(partB_2015_df.provider_type == p_type) & (partB_2015_df.average_medicare_payment_amt!=0)]['average_medicare_payment_amt']\n",
    "    fraud_x = partB_2015_df[(partB_2015_df.provider_type == p_type) & (partB_2015_df.fraud_average_medicare_payment_amt!=0)]['fraud_average_medicare_payment_amt']\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    sns.distplot(x,label='Non-fraudulent', hist=False, rug=False)\n",
    "    sns.distplot(fraud_x,label='Fraudulent', hist=False, rug=False)\n",
    "    ax.set(\n",
    "        title  ='Distribution of Medicare Payment Amount - Fraud vs. Non-fraud - '+ p_type,\n",
    "        xlabel = 'Log of USD Payment Amount',\n",
    "        ylabel = 'Count'\n",
    "          )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same conclusion can be made here about Medicare Payment amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlation1'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partB_2015_df = partB_df[partB_df['year'] == datetime(year=2015,month=1,day=1)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "sns.heatmap(partB_2015_df.corr(method='pearson'), annot=True, fmt='.4f', \n",
    "            cmap=plt.get_cmap('coolwarm'), cbar=False, robust=True)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=\"horizontal\")\n",
    "ax.set(\n",
    "    title  ='Correlation Heatmap in 2015 (only Continuous Variables)',\n",
    "\n",
    "      )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No insights are particularly interesting here, outside of the correlations we would normally expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical variables (except location)\n",
    "for col in ['nppes_provider_gender','provider_type']:\n",
    "    partB_2015_df = pd.concat([partB_2015_df, pd.get_dummies(partB_2015_df[col], drop_first= True)], axis=1)\n",
    "    partB_2015_df = partB_2015_df.drop(col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get duplicate pairs to drop in correlation matrix after unstacking'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_correlations(df):\n",
    "    '''Get biggest correlations'''\n",
    "    au_corr = df.corr().unstack() #unstack\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop)\n",
    "    return au_corr\n",
    "\n",
    "#get relevant columns\n",
    "cols = partB_2015_df.columns[20:].tolist() + ['line_srvc_cnt','bene_unique_cnt','average_submitted_chrg_amt','fraudulent']\n",
    "corr = get_correlations(partB_2015_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlation2'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr.to_frame().reset_index() #to frame\n",
    "corr.columns = ['Variable A', 'Variable B', 'Correlation']\n",
    "\n",
    "def color_df(value):\n",
    "    '''Color red if positive, green if negative'''\n",
    "    if value < 0:\n",
    "        color = 'red'\n",
    "    elif value > 0:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "corr = corr.reindex(corr['Correlation'].abs().sort_values(ascending=False).index).reset_index(drop=True)\n",
    "corr[:15].style.applymap(color_df, subset=['Correlation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='barchart1'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "col_layout_dict = {'non_fraud_services_count': ['Non-fraud Service','rgba(50, 171, 96, 0.6)'],\n",
    "                 'fraud_services_count': ['Fraud Service','rgb(255, 0, 0)']} #dict for layout\n",
    "\n",
    "for col in ['non_fraud_services_count','fraud_services_count']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=type_df['provider_type'],\n",
    "        x=type_df[col],\n",
    "        name=col_layout_dict[col][0],\n",
    "        marker=dict(\n",
    "            color=col_layout_dict[col][1],\n",
    "        ),\n",
    "        orientation='h',\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode = 'stack',\n",
    "    title = 'Top 15 Fraudulent Medicare Category by Service Count',\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "        domain=[0, 0.95],\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "        showgrid=True,\n",
    "        domain=[0, 0.95],\n",
    "    ),\n",
    "    xaxis_title_text='Count',\n",
    ")\n",
    "\n",
    "annotations = [] #annotate with %\n",
    "\n",
    "x   = type_df['fraud_services_count']+type_df['non_fraud_services_count']+100000000\n",
    "y_p = np.round(type_df['fraud_services_pct'].tolist(), decimals=2)\n",
    "\n",
    "for y_p, x, y in zip(y_p,x,type_df['provider_type']):\n",
    "    annotations.append(dict(xref='x1', yref='y1',\n",
    "                            y=y, x=x,\n",
    "                            text=str(y_p) + '%',\n",
    "                            font=dict(family='Arial', size=12,\n",
    "                                      color='rgb(255, 0, 0)'),\n",
    "                            showarrow=False))\n",
    "\n",
    "annotations.append(dict(xref='paper', yref='paper',\n",
    "                        x=-0.2, y=-0.209,\n",
    "                        text='Combined CMS and LEIE data' +\n",
    "                             'to label the leading Fraudulent physician categories (15 Feb 2020)',\n",
    "                        font=dict(family='Arial', size=10, color='rgb(150,150,150)'),\n",
    "                        showarrow=False))\n",
    "\n",
    "fig.update_layout(annotations=annotations)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='barchart2'></a>\n",
    "<a href='#menu'>[Menu]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "#dict for layout\n",
    "col_layout_dict = {'female_count': ['Female Physicians ','#ffcdd2'],\n",
    "                 'male_count': ['Male Physicians','#A2D5F2']}\n",
    "\n",
    "for col in ['female_count','male_count']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=type_df['provider_type'],\n",
    "        x=type_df[col],\n",
    "        name=col_layout_dict[col][0],\n",
    "        marker=dict(\n",
    "            color=col_layout_dict[col][1],\n",
    "        ),\n",
    "        orientation='h',\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode = 'stack',\n",
    "    title = 'Top 15 Fraudulent Medicare Category by Service Count and Gender',\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "        domain=[0, 0.90],\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        showticklabels=True,\n",
    "        showgrid=True,\n",
    "        domain=[0, 0.90],\n",
    "    ),\n",
    "    xaxis_title_text='Count',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical Laboratory and Ambulance Service Provider has a lot of line services but require fewer doctors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prep Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aggregate data following paper's method\n",
    "- Normalize predictors\n",
    "- One hot encoding\n",
    "- Write data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partB_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables that we are not using\n",
    "partB_train_df = partB_df.drop(columns=[\n",
    "    'fraud_average_medicare_payment_amt', #for visual only\n",
    "    'fraud_average_submitted_chrg_amt', #for visual only\n",
    "    'fraud_line_srvc_cnt', #for visual only\n",
    "    'year',\n",
    "    'excl_year',\n",
    "    'hcpcs_drug_indicator',\n",
    "    'place_of_service',\n",
    "    'nppes_provider_city',\n",
    "    'nppes_provider_country',\n",
    "    'hcpcs_description',\n",
    "    'nppes_provider_zip'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(partB_train_df.head())\n",
    "print(\"We have 9 features, including 4 categorical variables in \\n {0}\".format(partB_train_df.columns[1:5].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = partB_train_df.fraudulent.value_counts()\n",
    "display(a)\n",
    "print('Fraudulent physicians are: {0}% of all data'.format(str(np.round((a[1]/a[0])*100,decimals = 6)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "for col in ['provider_type', 'nppes_provider_state', 'hcpcs_code', 'nppes_provider_gender']:\n",
    "    partB_train_df = pd.concat([partB_train_df, pd.get_dummies(partB_train_df[col], drop_first= True)], axis=1)\n",
    "    partB_train_df = partB_train_df.drop(col, 1) #drop column that's been encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(agg_partB_df.shape)\n",
    "print('There are {0} predictors.'.format(agg_partB_df.shape[1]-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='norm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USING\n",
    "# Normalize predictors to [0,1] min-max scale\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def scale_predictors(df):\n",
    "    '''Takes in df and returns normalized data [0,1] '''\n",
    "    x  = df.values #numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    return pd.DataFrame(x_scaled, columns=df.columns, index=df.index)\n",
    "\n",
    "# scl_predict_df = scale_predictors(partB_train_df.iloc[:,7:(partB_train_df.shape[1]-1)])\n",
    "\n",
    "# # merge back in with npi and label\n",
    "# partB_train_df['npi']  = partB_train_df['npi_'].astype(str)\n",
    "# partB_train_df['year'] = partB_train_df['year_'].astype(str).str[:4]\n",
    "\n",
    "# partB_train_df = pd.concat([partB_train_df.iloc[:,[0,1,2,6,37]], scl_predict_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#menu'>[Menu]</a>\n",
    "<a id='ros'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data \n",
    "num_rows = 3200  # --> 20000 rows ~ 100 MB (limit on AutoAI)\n",
    "percent_fraud = 0.2  # TODO: Change this value to increase the percentage of AutoAI sample that is fraud \n",
    "n_fraud = min(int(num_rows * percent_fraud), agg_partB_df_fraud_filtered['fraudulent_mean'].count())\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "auto_ai_df = pd.concat([agg_partB_df_fraud_filtered.sample(n=n_fraud, random_state=random_state),\n",
    "                       agg_partB_df_not_fraud_filtered.sample(n=(num_rows-n_fraud), random_state=random_state)])\n",
    "print(f'Sample breakdown: {n_fraud} ({100*(n_fraud/num_rows):.2f}%) Fraud & {num_rows-n_fraud} ({100*((num_rows-n_fraud)/num_rows):.2f}%) Not Fraud')\n",
    "auto_ai_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
